{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beingaryan/Construction-Accident-Analysis/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKsUk_MG7f1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRKFN19f7f1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea426b45-0db5-40fc-d44d-d317d862e809"
      },
      "source": [
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT2T9RQz7f1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8uPWvQK7f10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO_oRE9d7f13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "5df71b7a-4811-4786-dfc1-e56b3ab6874a"
      },
      "source": [
        "df=pd.read_csv('./sample_data/tagged1000.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>Summary2</th>\n",
              "      <th>cause</th>\n",
              "      <th>newkeys</th>\n",
              "      <th>title.new</th>\n",
              "      <th>summary.new</th>\n",
              "      <th>Tagged2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>200361855</td>\n",
              "      <td>Two Workers Are Struck By Motor Vehicle And O...</td>\n",
              "      <td>On August 27 2013 Employees #1 and #2 of Templ...</td>\n",
              "      <td>Other</td>\n",
              "      <td>\\nconstruction, undrgrd power line, highway, ...</td>\n",
              "      <td>two workers are struck by motor vehicle and o...</td>\n",
              "      <td>On august 27 2013 employees #1 and #2 of templ...</td>\n",
              "      <td>traffic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>202673471</td>\n",
              "      <td>Foreman Is Fatally Crushed When Forklift Tips...</td>\n",
              "      <td>At approximately 6:30 a.m. on May 13 2013 Empl...</td>\n",
              "      <td>[]</td>\n",
              "      <td>\\nconstruction, equipment operator, industria...</td>\n",
              "      <td>foreman is fatally crushed when forklift tips...</td>\n",
              "      <td>At approximately 6:30 a.m. on may 13 2013 empl...</td>\n",
              "      <td>collapse of object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>202509832</td>\n",
              "      <td>Employee Suffers Abdominal Fracture In Fall F...</td>\n",
              "      <td>On April 9 2013 Employee #1 was installing vin...</td>\n",
              "      <td>Fall from/with ladder</td>\n",
              "      <td>\\ninstalling, ladder, scaffold, structure mov...</td>\n",
              "      <td>employee suffers abdominal fracture in fall f...</td>\n",
              "      <td>On april 9 2013 employee #1 was installing vin...</td>\n",
              "      <td>falls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>201562840</td>\n",
              "      <td>Employee'S Body Is Caught In Asphalt Machine ...</td>\n",
              "      <td>On November 27 2012 Employee #1 was operating ...</td>\n",
              "      <td>Crushed/run</td>\n",
              "      <td>\\nasphalt, machine operator, caught by, cloth...</td>\n",
              "      <td>employee's body is caught in asphalt machine ...</td>\n",
              "      <td>On november 27 2012 employee #1 was operating ...</td>\n",
              "      <td>caught in/between objects</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>202478632</td>\n",
              "      <td>Employee Is Punctured In Abdomen With Nail</td>\n",
              "      <td>At approximately 11:57 a.m. on September 28 20...</td>\n",
              "      <td>Other</td>\n",
              "      <td>\\nslip, nail, puncture, abdomen, fall\\n</td>\n",
              "      <td>employee is punctured in abdomen with nail</td>\n",
              "      <td>At approximately 11:57 a.m. on september 28 20...</td>\n",
              "      <td>struck by moving objects</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                    Tagged2\n",
              "0           0  ...                    traffic\n",
              "1           1  ...         collapse of object\n",
              "2           2  ...                      falls\n",
              "3           3  ...  caught in/between objects\n",
              "4           4  ...   struck by moving objects\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MiqHCDvF7f15",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "b461061e-a4ed-445b-ec5f-c804a64fcbf8"
      },
      "source": [
        "#preprocessing of data\n",
        "import nltk\n",
        "import numpy as np\n",
        "import string\n",
        "from keras.preprocessing.text import text_to_word_sequence #for tokenization\n",
        "PUNCT_TO_REMOVE = string.punctuation #punctuation removal\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    abc = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "    result = ''.join(i for i in abc if not i.isdigit()) #removal of numbers\n",
        "    return result\n",
        "\n",
        "df[\"punctr\"] = df[\"Summary2\"].apply(lambda text: remove_punctuation(text)) #remove punctuation\n",
        "df.head()\n",
        "def tokenize(text):#for tokenization\n",
        "    tokens = text_to_word_sequence(text)\n",
        "    return tokens\n",
        "df['clean_tokens_summ']=df['punctr'].apply(lambda x: tokenize(x.lower()))\n",
        "df['clean_tokens_cause']=df['cause'].apply(lambda x: tokenize(x.lower()))\n",
        "import nltk\n",
        "#stopwords removal\n",
        "STOPWORDS = nltk.corpus.stopwords.words('english')+['june','january','february','march','april','may','july','august','september','october','november','december']\n",
        "def remove_stopwords(text):\n",
        "    cleantext = [word for word in text if word not in STOPWORDS]\n",
        "    return cleantext\n",
        "df[\"summ_wstp\"] = df[\"clean_tokens_summ\"].apply(lambda text: remove_stopwords(text))\n",
        "df.head()\n",
        "#lemmatization\n",
        "from nltk import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    clnt = [lemmatizer.lemmatize(word) for word in text]\n",
        "    return clnt\n",
        "df[\"summ_lmt\"] = df[\"summ_wstp\"].apply(lambda text: lemmatize_words(text))\n",
        "#POS TAGGING\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "def pos(text):\n",
        "    post=[nltk.pos_tag(word_tokenize(word)) for word in text]\n",
        "   \n",
        "    return post\n",
        "df[\"summ_post\"] = df[\"summ_lmt\"].apply(lambda text: pos(text))\n",
        "df['summ_post'].iloc[8]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('employee', 'NN')],\n",
              " [('employed', 'VBN')],\n",
              " [('construction', 'NN')],\n",
              " [('company', 'NN')],\n",
              " [('roofing', 'VBG')],\n",
              " [('work', 'NN')],\n",
              " [('new', 'JJ')],\n",
              " [('home', 'NN')],\n",
              " [('construction', 'NN')],\n",
              " [('installing', 'VBG')],\n",
              " [('roofing', 'VBG')],\n",
              " [('felt', 'NN')],\n",
              " [('paper', 'NN')],\n",
              " [('walked', 'VBD')],\n",
              " [('backwards', 'NNS')],\n",
              " [('fell', 'VBD')],\n",
              " [('roof', 'NN')],\n",
              " [('landed', 'VBD')],\n",
              " [('ground', 'NN')],\n",
              " [('distance', 'NN')],\n",
              " [('foot', 'NN')],\n",
              " [('emergency', 'NN')],\n",
              " [('service', 'NN')],\n",
              " [('called', 'VBN')],\n",
              " [('employee', 'NN')],\n",
              " [('transported', 'VBN')],\n",
              " [('hospital', 'NN')],\n",
              " [('treated', 'VBN')],\n",
              " [('fracture', 'NN')],\n",
              " [('left', 'NN')],\n",
              " [('arm', 'NN')],\n",
              " [('scrape', 'NN')],\n",
              " [('left', 'NN')],\n",
              " [('side', 'NN')],\n",
              " [('face', 'NN')],\n",
              " [('hospitalized', 'VBN')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD7a9auk7f18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "1bd54a0f-3999-4c9e-bfaf-2cf344f065ed"
      },
      "source": [
        "# arr=df['summ_post'].to_numpy()\n",
        "# # arr.flatten()\n",
        "# print(arr)\n",
        "df['summ_lmt'].iloc[8]\n",
        "# # df['summ_lmt'].iloc[8]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['employee',\n",
              " 'employed',\n",
              " 'construction',\n",
              " 'company',\n",
              " 'roofing',\n",
              " 'work',\n",
              " 'new',\n",
              " 'home',\n",
              " 'construction',\n",
              " 'installing',\n",
              " 'roofing',\n",
              " 'felt',\n",
              " 'paper',\n",
              " 'walked',\n",
              " 'backwards',\n",
              " 'fell',\n",
              " 'roof',\n",
              " 'landed',\n",
              " 'ground',\n",
              " 'distance',\n",
              " 'foot',\n",
              " 'emergency',\n",
              " 'service',\n",
              " 'called',\n",
              " 'employee',\n",
              " 'transported',\n",
              " 'hospital',\n",
              " 'treated',\n",
              " 'fracture',\n",
              " 'left',\n",
              " 'arm',\n",
              " 'scrape',\n",
              " 'left',\n",
              " 'side',\n",
              " 'face',\n",
              " 'hospitalized']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-ST7X9K8_KR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "399e2fb0-fe33-4167-fbba-48d164a01217"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1JPLOck7f1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "39680581-7440-4e39-fad4-e5dbea56285d"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect=TfidfVectorizer()\n",
        "#     tokenizer=dummy_fun,\n",
        "#     preprocessor=dummy_fun,\n",
        "#     token_pattern=None)\n",
        "\"\"\"def lemmatize_words(text):\n",
        "    clnt = [lemmatizer.lemmatize(word) for word in text]\n",
        "    return clnt\n",
        "        return clnt\n",
        "df[\"summ_lmt\"] = df[\"summ_wstp\"].apply(lambda text: lemmatize_words(text))\"\"\"\n",
        "def vectorizer(text):\n",
        "    temp=[]\n",
        "#     text.split(\",\")\n",
        "    for texts in text:\n",
        "         temp.append(tfidf_vect.fit_transform(texts))\n",
        "#         for words in texts:\n",
        "           \n",
        "    return temp\n",
        "            \n",
        "#     rest = [for word in text]\n",
        "#     rest=(tfidf_vect.fit_transform(word) for word in rest)\n",
        "#     return rest\n",
        "    \n",
        "df[\"summ_tfidf\"] = df[\"summ_lmt\"].apply(lambda text: vectorizer(text))\n",
        "#X = tfidf_vect.fit_transform(df[\"summ_post\"])\n",
        "# print(X.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4fcecde40370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     return rest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summ_tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summ_lmt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m#X = tfidf_vect.fit_transform(df[\"summ_post\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# print(X.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-4fcecde40370>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     return rest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summ_tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summ_lmt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m#X = tfidf_vect.fit_transform(df[\"summ_post\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# print(X.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-4fcecde40370>\u001b[0m in \u001b[0;36mvectorizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     text.split(\",\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m          \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#         for words in texts:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1210\u001b[0;31m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m                 \"string object received.\")\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZkmAD0U7f2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "def tokenize(text):\n",
        "    stem = nltk.stem.SnowballStemmer('english')\n",
        "    text = text.lower()\n",
        "\n",
        "    for token in nltk.word_tokenize(text):\n",
        "        if token in string.punctuation: continue\n",
        "        yield stem.stem(token)\n",
        "\n",
        "corpus = [\n",
        "    \"The elephant sneezed at the sight of potatoes.\",\n",
        "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
        "    \"Wondering, she opened the door to the studio.\",\n",
        "]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCX24qzG7f2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf  = TfidfVectorizer()\n",
        "# corpus = [\n",
        "#     \"The elephant sneezed at the sight of potatoes.\",\n",
        "#     \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
        "#     \"Wondering, she opened the door to the studio.\",\n",
        "# ]\n",
        "corpus = tfidf.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE2pkaNx7f2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "6aab45b2-213a-4d2e-8790-9d8645eaf4ca"
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 9)\t0.37867626873820165\n",
            "  (0, 7)\t0.37867626873820165\n",
            "  (0, 12)\t0.28799306292785165\n",
            "  (0, 0)\t0.37867626873820165\n",
            "  (0, 14)\t0.37867626873820165\n",
            "  (0, 6)\t0.37867626873820165\n",
            "  (0, 16)\t0.44730460893892116\n",
            "  (1, 13)\t0.30251368128649075\n",
            "  (1, 1)\t0.30251368128649075\n",
            "  (1, 5)\t0.30251368128649075\n",
            "  (1, 18)\t0.30251368128649075\n",
            "  (1, 10)\t0.6050273625729815\n",
            "  (1, 3)\t0.30251368128649075\n",
            "  (1, 2)\t0.30251368128649075\n",
            "  (1, 12)\t0.23006945204561577\n",
            "  (1, 16)\t0.1786694534059618\n",
            "  (2, 15)\t0.3677238693250534\n",
            "  (2, 17)\t0.3677238693250534\n",
            "  (2, 4)\t0.3677238693250534\n",
            "  (2, 8)\t0.3677238693250534\n",
            "  (2, 11)\t0.3677238693250534\n",
            "  (2, 19)\t0.3677238693250534\n",
            "  (2, 16)\t0.4343672818844283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obmqyZQR7f2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a=df['summ_tfidf'].iloc[0]\n",
        "\n",
        "# from scipy.sparse import csr_matrix\n",
        "# csr=csr_matrix(a.any())\n",
        "# csr.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChywNtYf7f2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "203acaa6-0eeb-4452-b408-1969f6ccf748"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>Summary2</th>\n",
              "      <th>cause</th>\n",
              "      <th>newkeys</th>\n",
              "      <th>title.new</th>\n",
              "      <th>summary.new</th>\n",
              "      <th>Tagged2</th>\n",
              "      <th>punctr</th>\n",
              "      <th>clean_tokens_summ</th>\n",
              "      <th>clean_tokens_cause</th>\n",
              "      <th>summ_wstp</th>\n",
              "      <th>summ_lmt</th>\n",
              "      <th>summ_post</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>200361855</td>\n",
              "      <td>Two Workers Are Struck By Motor Vehicle And O...</td>\n",
              "      <td>On August 27 2013 Employees #1 and #2 of Templ...</td>\n",
              "      <td>Other</td>\n",
              "      <td>\\nconstruction, undrgrd power line, highway, ...</td>\n",
              "      <td>two workers are struck by motor vehicle and o...</td>\n",
              "      <td>On august 27 2013 employees #1 and #2 of templ...</td>\n",
              "      <td>traffic</td>\n",
              "      <td>On August   Employees  and  of Templar Inc a c...</td>\n",
              "      <td>[on, august, employees, and, of, templar, inc,...</td>\n",
              "      <td>[other]</td>\n",
              "      <td>[employees, templar, inc, construction, compan...</td>\n",
              "      <td>[employee, templar, inc, construction, company...</td>\n",
              "      <td>[[(employee, NN)], [(templar, NN)], [(inc, NN)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>202673471</td>\n",
              "      <td>Foreman Is Fatally Crushed When Forklift Tips...</td>\n",
              "      <td>At approximately 6:30 a.m. on May 13 2013 Empl...</td>\n",
              "      <td>[]</td>\n",
              "      <td>\\nconstruction, equipment operator, industria...</td>\n",
              "      <td>foreman is fatally crushed when forklift tips...</td>\n",
              "      <td>At approximately 6:30 a.m. on may 13 2013 empl...</td>\n",
              "      <td>collapse of object</td>\n",
              "      <td>At approximately  am on May   Employee  a fore...</td>\n",
              "      <td>[at, approximately, am, on, may, employee, a, ...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[approximately, employee, foreman, regularly, ...</td>\n",
              "      <td>[approximately, employee, foreman, regularly, ...</td>\n",
              "      <td>[[(approximately, RB)], [(employee, NN)], [(fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>202509832</td>\n",
              "      <td>Employee Suffers Abdominal Fracture In Fall F...</td>\n",
              "      <td>On April 9 2013 Employee #1 was installing vin...</td>\n",
              "      <td>Fall from/with ladder</td>\n",
              "      <td>\\ninstalling, ladder, scaffold, structure mov...</td>\n",
              "      <td>employee suffers abdominal fracture in fall f...</td>\n",
              "      <td>On april 9 2013 employee #1 was installing vin...</td>\n",
              "      <td>falls</td>\n",
              "      <td>On April   Employee  was installing vinyl sidi...</td>\n",
              "      <td>[on, april, employee, was, installing, vinyl, ...</td>\n",
              "      <td>[fall, from, with, ladder]</td>\n",
              "      <td>[employee, installing, vinyl, sidings, single,...</td>\n",
              "      <td>[employee, installing, vinyl, siding, single, ...</td>\n",
              "      <td>[[(employee, NN)], [(installing, VBG)], [(viny...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>201562840</td>\n",
              "      <td>Employee'S Body Is Caught In Asphalt Machine ...</td>\n",
              "      <td>On November 27 2012 Employee #1 was operating ...</td>\n",
              "      <td>Crushed/run</td>\n",
              "      <td>\\nasphalt, machine operator, caught by, cloth...</td>\n",
              "      <td>employee's body is caught in asphalt machine ...</td>\n",
              "      <td>On november 27 2012 employee #1 was operating ...</td>\n",
              "      <td>caught in/between objects</td>\n",
              "      <td>On November   Employee  was operating an aspha...</td>\n",
              "      <td>[on, november, employee, was, operating, an, a...</td>\n",
              "      <td>[crushed, run]</td>\n",
              "      <td>[employee, operating, asphaltpulverizing, mach...</td>\n",
              "      <td>[employee, operating, asphaltpulverizing, mach...</td>\n",
              "      <td>[[(employee, NN)], [(operating, NN)], [(asphal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>202478632</td>\n",
              "      <td>Employee Is Punctured In Abdomen With Nail</td>\n",
              "      <td>At approximately 11:57 a.m. on September 28 20...</td>\n",
              "      <td>Other</td>\n",
              "      <td>\\nslip, nail, puncture, abdomen, fall\\n</td>\n",
              "      <td>employee is punctured in abdomen with nail</td>\n",
              "      <td>At approximately 11:57 a.m. on september 28 20...</td>\n",
              "      <td>struck by moving objects</td>\n",
              "      <td>At approximately  am on September   Employee  ...</td>\n",
              "      <td>[at, approximately, am, on, september, employe...</td>\n",
              "      <td>[other]</td>\n",
              "      <td>[approximately, employee, working, coworker, j...</td>\n",
              "      <td>[approximately, employee, working, coworker, j...</td>\n",
              "      <td>[[(approximately, RB)], [(employee, NN)], [(wo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                          summ_post\n",
              "0           0  ...  [[(employee, NN)], [(templar, NN)], [(inc, NN)...\n",
              "1           1  ...  [[(approximately, RB)], [(employee, NN)], [(fo...\n",
              "2           2  ...  [[(employee, NN)], [(installing, VBG)], [(viny...\n",
              "3           3  ...  [[(employee, NN)], [(operating, NN)], [(asphal...\n",
              "4           4  ...  [[(approximately, RB)], [(employee, NN)], [(wo...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuqF4omz7f2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}